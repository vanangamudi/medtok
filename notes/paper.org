:PROPERTIES:
:ID:       b28af6a7-7039-4021-8c7d-7a9e422e89b2
:END:
#+title: Paper Reading - Multimodal Medical Code Tokenizer - 2502.04397
[[https://arxiv.org/pdf/2502.04397][
Multimodal Medical Code Tokenizer]]
* Notes
- The tokenization described in that paper is not typical tokenization. They use a very liberal definition of tokenization. We need to read at least two papers one on vector quantized tokenization and one that is information criterion controlled disentanglement of multi modal.

* Problems with standard tokenization
Standard tokenization strategies inherited from general-purpose language models fail to capture the complexity of medical codes, leading to six key challenges:
- (1) Scalability of medical vocabularies – Medical coding systems contain over 600,000 unique codes, far exceeding standard tokenizer capacities. Treating each code as a separate token leads to inefficient vocabulary expansion, increasing memory demands and fragmenting rare codes (e.g., splitting ”ICD9: 250.0” into arbitrary subwords).
- (2) Loss of hierarchical and relational structure – Many coding systems encode structured dependencies, such as ATC codes, which classify drugs based on pharmacological and chemical properties (Miller & Britt, 1995).  Standard tokenizers, relying only on co-occurrence statistics, fail to capture hierarchical relationships, losing dependencies like disease co-occurrences and drug contraindications.
- (3) Redundancy across coding systems – Identical clinical concepts often appear under different codes across terminologies (e.g., ICD vs. SNOMED). Standard tokenization treats them as separate tokens, creating redundancy and complicating cross-system data integration.
- (4) Inefficiency in token storage – Expanding vocabulary sizes to accom- modate medical codes results in bloated embedding tables that degrade computational efficiency, particularly for low-resource codes that appear infrequently but still require dedicated tokens.
- (5) Sparse and inconsistent usage – Many medical codes are rarely used or inconsistently documented, making it difficult for standard tokenizers to learn meaning- ful representations. Low-frequency codes suffer from poor embeddings, reducing performance on underrepresented conditions.
- (6) Lack of multimodal representations – Existing methods (Jiang et al., 2023b; Zhu et al., 2024; Xu et al., 2024) treat medical codes as isolated textual tokens, discarding graph-based relationships that encode essential links between diagnoses, treatments, and medications. A robust tokenizer must integrate both textual and relational information to fully represent medical codes
* Works surrounding it
** [[https://arxiv.org/abs/2407.09087][[2407.09087] On the Role of Discrete Tokenization in Visual Representation Learning]]
 Abstract:In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. In this paper, we explore the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between MIM and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. Furthermore, we propose a novel metric named TCAS, which is specifically designed to assess the effectiveness of discrete tokens within the MIM framework. Inspired by this metric, we contribute an innovative tokenizer design and propose a corresponding MIM method named ClusterMIM. It demonstrates superior performance on a variety of benchmark datasets and ViT backbones. Code is available at [[https://github.com/PKU-ML/ClusterMIM][this https URL]].

** [[https://arxiv.org/abs/2410.23996][[2410.23996] An Information Criterion for Controlled Disentanglement of Multimodal Data]]
  Multimodal representation learning seeks to relate and decompose information inherent in multiple modalities. By disentangling modality-specific information from information that is shared across modalities, we can improve interpretability and robustness and enable downstream tasks such as the generation of counterfactual outcomes. Separating the two types of information is challenging since they are often deeply entangled in many real-world applications. We propose Disentangled Self-Supervised Learning (DisentangledSSL), a novel self-supervised approach for learning disentangled representations. We present a comprehensive analysis of the optimality of each disentangled representation, particularly focusing on the scenario not covered in prior work where the so-called Minimum Necessary Information (MNI) point is not attainable. We demonstrate that DisentangledSSL successfully learns shared and modality-specific features on multiple synthetic and real-world datasets and consistently outperforms baselines on various downstream tasks, including prediction tasks for vision-language data, as well as molecule-phenotype retrieval tasks for biological data.

** [[https://arxiv.org/abs/1711.00937][[1711.00937] Neural Discrete Representation Learning]]
Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.

** [[https://arxiv.org/abs/2212.03185][[2212.03185] Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis]]

  Vector-Quantized (VQ-based) generative models usually consist of two basic components, i.e., VQ tokenizers and generative transformers. Prior research focuses on improving the reconstruction fidelity of VQ tokenizers but rarely examines how the improvement in reconstruction affects the generation ability of generative transformers. In this paper, we surprisingly find that improving the reconstruction fidelity of VQ tokenizers does not necessarily improve the generation. Instead, learning to compress semantic features within VQ tokenizers significantly improves generative transformers' ability to capture textures and structures. We thus highlight two competing objectives of VQ tokenizers for image synthesis: semantic compression and details preservation. Different from previous work that only pursues better details preservation, we propose Semantic-Quantized GAN (SeQ-GAN) with two learning phases to balance the two objectives. In the first phase, we propose a semantic-enhanced perceptual loss for better semantic compression. In the second phase, we fix the encoder and codebook, but enhance and finetune the decoder to achieve better details preservation. The proposed SeQ-GAN greatly improves VQ-based generative models and surpasses the GAN and Diffusion Models on both unconditional and conditional image generation. Our SeQ-GAN (364M) achieves Frechet Inception Distance (FID) of 6.25 and Inception Score (IS) of 140.9 on 256x256 ImageNet generation, a remarkable improvement over VIT-VQGAN (714M), which obtains 11.2 FID and 97.2 IS.

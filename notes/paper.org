:PROPERTIES:
:ID:       b28af6a7-7039-4021-8c7d-7a9e422e89b2
:END:
#+title: Paper Reading - Multimodal Medical Code Tokenizer - 2502.04397
[[https://arxiv.org/pdf/2502.04397][
Multimodal Medical Code Tokenizer]]
* Notes
- The tokenization described in that paper is not typical tokenization. They use a very liberal definition of tokenization. We need to read at least two papers one on vector quantized tokenization and one that is information criterion controlled disentanglement of multi modal.

* Problems with standard tokenization
Standard tokenization strategies inherited from general-purpose language models fail to capture the complexity of medical codes, leading to six key challenges:
- (1) Scalability of medical vocabularies – Medical coding systems contain over 600,000 unique codes, far exceeding standard tokenizer capacities. Treating each code as a separate token leads to inefficient vocabulary expansion, increasing memory demands and fragmenting rare codes (e.g., splitting ”ICD9: 250.0” into arbitrary subwords).
- (2) Loss of hierarchical and relational structure – Many coding systems encode structured dependencies, such as ATC codes, which classify drugs based on pharmacological and chemical properties (Miller & Britt, 1995).  Standard tokenizers, relying only on co-occurrence statistics, fail to capture hierarchical relationships, losing dependencies like disease co-occurrences and drug contraindications.
- (3) Redundancy across coding systems – Identical clinical concepts often appear under different codes across terminologies (e.g., ICD vs. SNOMED). Standard tokenization treats them as separate tokens, creating redundancy and complicating cross-system data integration.
- (4) Inefficiency in token storage – Expanding vocabulary sizes to accom- modate medical codes results in bloated embedding tables that degrade computational efficiency, particularly for low-resource codes that appear infrequently but still require dedicated tokens.
- (5) Sparse and inconsistent usage – Many medical codes are rarely used or inconsistently documented, making it difficult for standard tokenizers to learn meaning- ful representations. Low-frequency codes suffer from poor embeddings, reducing performance on underrepresented conditions.
- (6) Lack of multimodal representations – Existing methods (Jiang et al., 2023b; Zhu et al., 2024; Xu et al., 2024) treat medical codes as isolated textual tokens, discarding graph-based relationships that encode essential links between diagnoses, treatments, and medications. A robust tokenizer must integrate both textual and relational information to fully represent medical codes
* Works surrounding it
** [[https://arxiv.org/abs/2407.09087][[2407.09087] On the Role of Discrete Tokenization in Visual Representation Learning]]
 Abstract:In the realm of self-supervised learning (SSL), masked image modeling (MIM) has gained popularity alongside contrastive learning methods. MIM involves reconstructing masked regions of input images using their unmasked portions. A notable subset of MIM methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. In this paper, we explore the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between MIM and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. Furthermore, we propose a novel metric named TCAS, which is specifically designed to assess the effectiveness of discrete tokens within the MIM framework. Inspired by this metric, we contribute an innovative tokenizer design and propose a corresponding MIM method named ClusterMIM. It demonstrates superior performance on a variety of benchmark datasets and ViT backbones. Code is available at [[https://github.com/PKU-ML/ClusterMIM][this https URL]].

*** Notes
In the context of Masked Image Modeling (MIM), discrete tokens refer to quantized representations of image patches instead of using raw pixel values or continuous features. These tokens act as symbolic units that simplify the reconstruction task by mapping image patches into a predefined set of discrete categories.
- Tokenization Process ::
  - An image is divided into small patches (e.g., 16×16 pixels).
  - Each patch is converted into a discrete token using a visual tokenizer (e.g., VQGAN, dVAE).
  - The tokens serve as a target representation for the model to reconstruct the missing (masked) parts of the image.
  - Examples :
    - dVAE: Compresses image patches into a finite set of token indices.
    - VQGAN: Maps patches into a learned codebook of discrete representations.
    - Perceptual Tokenizers: Improve token quality by incorporating perceptual loss for better feature alignment.
    - ClusterMIM(this work): Uses K-means clustering to assign patches to discrete groups.

- Why Use Discrete Tokens? ::
  - Instead of reconstructing pixel values, the model predicts which token a missing patch belongs to.
  - This makes learning more efficient by reducing output complexity and helping the model focus on semantic structures rather than pixel-level details.

Discrete tokens in MIM are compact, symbolic representations of image patches, generated by visual tokenizers, that serve as reconstruction targets in self-supervised learning.The study finds that well-designed discrete tokens (aligned with semantic classes) improve generalization and help MIM models learn better representations. Conversely, poor tokenization can introduce inter-class confusion and hurt downstream performance.

** [[https://arxiv.org/abs/2410.23996][[2410.23996] An Information Criterion for Controlled Disentanglement of Multimodal Data]]
  Multimodal representation learning seeks to relate and decompose information inherent in multiple modalities. By disentangling modality-specific information from information that is shared across modalities, we can improve interpretability and robustness and enable downstream tasks such as the generation of counterfactual outcomes. Separating the two types of information is challenging since they are often deeply entangled in many real-world applications. We propose Disentangled Self-Supervised Learning (DisentangledSSL), a novel self-supervised approach for learning disentangled representations. We present a comprehensive analysis of the optimality of each disentangled representation, particularly focusing on the scenario not covered in prior work where the so-called Minimum Necessary Information (MNI) point is not attainable. We demonstrate that DisentangledSSL successfully learns shared and modality-specific features on multiple synthetic and real-world datasets and consistently outperforms baselines on various downstream tasks, including prediction tasks for vision-language data, as well as molecule-phenotype retrieval tasks for biological data.

** [[https://arxiv.org/abs/1711.00937][[1711.00937] Neural Discrete Representation Learning]]
Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.
*** Notes
- Gumbel-softmax
 #+begin_quote
 Recently a few authors have suggested the use of a new continuous reparemetrisation based on the so-called Concrete or Gumbel-softmax distribution, which is a continuous distribution and has a temperature constant that can be annealed during training to converge to a discrete distribution in the limit. In the beginning of training the variance of the gradients is low but biased, and towards the end of training the variance becomes high but unbiased.
 #+end_quote
- VQ-VAE code: https://github.com/google-deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb
** [[https://arxiv.org/abs/1901.03416][{1901.03416} Preventing Posterior Collapse with delta-VAEs]]
** [[https://arxiv.org/abs/2212.03185][[2212.03185] Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis]]
Vector-Quantized (VQ-based) generative models usually consist of two basic components, i.e., VQ tokenizers and generative transformers. Prior research focuses on improving the reconstruction fidelity of VQ tokenizers but rarely examines how the improvement in reconstruction affects the generation ability of generative transformers. In this paper, we surprisingly find that improving the reconstruction fidelity of VQ tokenizers does not necessarily improve the generation. Instead, learning to compress semantic features within VQ tokenizers significantly improves generative transformers' ability to capture textures and structures. We thus highlight two competing objectives of VQ tokenizers for image synthesis: /semantic compression and details preservation/. Different from previous work that only pursues better details preservation, we propose Semantic-Quantized GAN (SeQ-GAN) with two learning phases to balance the two objectives. In the first phase, we propose a semantic-enhanced perceptual loss for better semantic compression. In the second phase, we fix the encoder and codebook, but enhance and finetune the decoder to achieve better details preservation. The proposed SeQ-GAN greatly improves VQ-based generative models and surpasses the GAN and Diffusion Models on both unconditional and conditional image generation. Our SeQ-GAN (364M) achieves Frechet Inception Distance (FID) of 6.25 and Inception Score (IS) of 140.9 on 256x256 ImageNet generation, a remarkable improvement over VIT-VQGAN (714M), which obtains 11.2 FID and 97.2 IS.

** VQGAN
- VQ improves the quality of generated images
- also making it easier to manipulate specific aspects of them.
- it splits the images into smaller discrete components that each represent a specific aspect, NxN patch -> codebook entry
- codebook: the finite number of possible components.

- Note ::
  One recent, commonly used model that quantizes images into integer tokens is the [[https://arxiv.org/abs/1711.00937][Vector-quantized Variational AutoEncoder]] (VQVAE), a CNN-based auto-encoder whose latent space is a matrix of discrete learnable variables, trained end-to-end. [[https://arxiv.org/abs/2012.09841][VQGAN]] is an improved version of this that introduces an [[https://arxiv.org/abs/1406.2661][adversarial loss]] to promote high quality reconstruction. VQGAN uses transformer-like elements in the form of non-local attention blocks, which allows it to capture distant interactions using fewer layers.
